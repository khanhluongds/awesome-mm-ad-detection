# 🧠 Awesome Multi-modal Deceptive Ad Detection

This repository collects key papers, tools, and datasets relevant to political ad transparency, multi-modal deception detection, and vision-language models.

---

## 🔧 Foundational Multi-modal Models

### [CLIP (Radford et al., 2021)](https://arxiv.org/abs/2103.00020)
> A contrastive vision-language model trained on 400M image-text pairs. It enables zero-shot classification and text-image alignment without task-specific training.  
📄 [Detailed notes](./papers/clip.md)

### [BLIP-2 (Li et al., 2023)](https://arxiv.org/abs/2301.12597)
> Uses frozen image encoders + LLMs to enable better reasoning across modalities. Achieves strong performance in VQA and captioning tasks.  
📄 _(Coming soon)_

### [LLaVA (Liu et al., 2023)](https://llava-vl.github.io/)
> Visual instruction-tuned model that integrates vision and LLMs. Useful for explainable detection and visual-text QA.  
📄 _(Coming soon)_

---

## 🕵️ Deception & Ad Transparency

### [MisVis (2022)](https://arxiv.org/abs/2205.03594)
> Proposes a framework for detecting misleading image-text pairs by modeling deep semantic similarity. Evaluated on both factual and emotional mismatches.  
📄 [Detailed notes](./papers/misvis.md)

---

## 🗃️ Datasets

- [Meta Ad Library API](https://www.facebook.com/ads/library/api)  
  Provides political ads with region, spend, and targeting data.

- [Google Political Ads Transparency Report](https://transparencyreport.google.com/political-ads/home)  
  A global dataset of political ads from Google platforms.

